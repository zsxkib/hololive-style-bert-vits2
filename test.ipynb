{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pydantic\n",
    "# !pip install gradio==3.48.0\n",
    "# !pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up. Please be patient...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsakib/miniconda3/envs/hsbv2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Prediction interface for Cog ⚙️\n",
    "# https://cog.run/python\n",
    "\n",
    "from cog import BasePredictor, Path\n",
    "\n",
    "print(\"Starting up. Please be patient...\")\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional\n",
    "import json\n",
    "import utils\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import soundfile as sf\n",
    "\n",
    "from common.log import logger\n",
    "from common.tts_model import ModelHolder\n",
    "from infer import InvalidToneError\n",
    "from text.japanese import g2kata_tone, kata_tone2phone_tone, text_normalize\n",
    "\n",
    "is_hf_spaces = os.getenv(\"SYSTEM\") == \"spaces\"\n",
    "limit = 150\n",
    "\n",
    "# Get path settings\n",
    "with open(os.path.join(\"configs\", \"paths.yml\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    path_config: dict[str, str] = yaml.safe_load(f.read())\n",
    "    # dataset_root = path_config[\"dataset_root\"]\n",
    "    assets_root = path_config[\"assets_root\"]\n",
    "\n",
    "\n",
    "def tts_fn(\n",
    "    model_name,\n",
    "    model_path,\n",
    "    text,\n",
    "    language,\n",
    "    reference_audio_path,\n",
    "    sdp_ratio,\n",
    "    noise_scale,\n",
    "    noise_scale_w,\n",
    "    length_scale,\n",
    "    line_split,\n",
    "    split_interval,\n",
    "    assist_text,\n",
    "    assist_text_weight,\n",
    "    use_assist_text,\n",
    "    style,\n",
    "    style_weight,\n",
    "    kata_tone_json_str,\n",
    "    use_tone,\n",
    "    speaker,\n",
    "):\n",
    "    print(f\"[!] model_name: {model_name}\")\n",
    "    print(f\"[!] model_path: {model_path}\")\n",
    "    print(f\"[!] text: {text}\")\n",
    "    print(f\"[!] language: {language}\")\n",
    "    print(f\"[!] reference_audio_path: {reference_audio_path}\")\n",
    "    print(f\"[!] sdp_ratio: {sdp_ratio}\")\n",
    "    print(f\"[!] noise_scale: {noise_scale}\")\n",
    "    print(f\"[!] noise_scale_w: {noise_scale_w}\")\n",
    "    print(f\"[!] length_scale: {length_scale}\")\n",
    "    print(f\"[!] line_split: {line_split}\")\n",
    "    print(f\"[!] split_interval: {split_interval}\")\n",
    "    print(f\"[!] assist_text: {assist_text}\")\n",
    "    print(f\"[!] assist_text_weight: {assist_text_weight}\")\n",
    "    print(f\"[!] use_assist_text: {use_assist_text}\")\n",
    "    print(f\"[!] style: {style}\")\n",
    "    print(f\"[!] style_weight: {style_weight}\")\n",
    "    print(f\"[!] kata_tone_json_str: {kata_tone_json_str}\")\n",
    "    print(f\"[!] use_tone: {use_tone}\")\n",
    "    print(f\"[!] speaker: {speaker}\")\n",
    "    if len(text) < 2:\n",
    "        return \"Please enter some text.\", None, kata_tone_json_str\n",
    "\n",
    "    if is_hf_spaces and len(text) > limit:\n",
    "        return (\n",
    "            f\"Too long! There is a character limit of {limit} characters.\",\n",
    "            None,\n",
    "            kata_tone_json_str,\n",
    "        )\n",
    "\n",
    "    if not model_holder.current_model:\n",
    "        model_holder.load_model_gr(model_name, model_path)\n",
    "        logger.info(f\"Loaded model '{model_name}'\")\n",
    "    if model_holder.current_model.model_path != model_path:\n",
    "        model_holder.load_model_gr(model_name, model_path)\n",
    "        logger.info(f\"Swapped to model '{model_name}'\")\n",
    "    speaker_id = model_holder.current_model.spk2id[speaker]\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    wrong_tone_message = \"\"\n",
    "    kata_tone: Optional[list[tuple[str, int]]] = None\n",
    "    if use_tone and kata_tone_json_str != \"\":\n",
    "        if language != \"JP\":\n",
    "            # logger.warning(\"Only Japanese is supported for tone generation.\")\n",
    "            wrong_tone_message = \"アクセント指定は現在日本語のみ対応しています。\"\n",
    "        if line_split:\n",
    "            # logger.warning(\"Tone generation is not supported for line split.\")\n",
    "            wrong_tone_message = (\n",
    "                \"アクセント指定は改行で分けて生成を使わない場合のみ対応しています。\"\n",
    "            )\n",
    "        try:\n",
    "            kata_tone = []\n",
    "            json_data = json.loads(kata_tone_json_str)\n",
    "            # tupleを使うように変換\n",
    "            for kana, tone in json_data:\n",
    "                assert isinstance(kana, str) and tone in (0, 1), f\"{kana}, {tone}\"\n",
    "                kata_tone.append((kana, tone))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error occurred when parsing kana_tone_json: {e}\")\n",
    "            wrong_tone_message = f\"アクセント指定が不正です: {e}\"\n",
    "            kata_tone = None\n",
    "\n",
    "    # toneは実際に音声合成に代入される際のみnot Noneになる\n",
    "    tone: Optional[list[int]] = None\n",
    "    if kata_tone is not None:\n",
    "        phone_tone = kata_tone2phone_tone(kata_tone)\n",
    "        tone = [t for _, t in phone_tone]\n",
    "\n",
    "    try:\n",
    "        sr, audio = model_holder.current_model.infer(\n",
    "            text=text,\n",
    "            language=language,\n",
    "            reference_audio_path=reference_audio_path,\n",
    "            sdp_ratio=sdp_ratio,\n",
    "            noise=noise_scale,\n",
    "            noisew=noise_scale_w,\n",
    "            length=length_scale,\n",
    "            line_split=line_split,\n",
    "            split_interval=split_interval,\n",
    "            assist_text=assist_text,\n",
    "            assist_text_weight=assist_text_weight,\n",
    "            use_assist_text=use_assist_text,\n",
    "            style=style,\n",
    "            style_weight=style_weight,\n",
    "            given_tone=tone,\n",
    "            sid=speaker_id,\n",
    "        )\n",
    "    except InvalidToneError as e:\n",
    "        logger.error(f\"Tone error: {e}\")\n",
    "        return f\"Error: アクセント指定が不正です:\\n{e}\", None, kata_tone_json_str\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Value error: {e}\")\n",
    "        return f\"Error: {e}\", None, kata_tone_json_str\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "    if tone is None and language == \"JP\":\n",
    "        # アクセント指定に使えるようにアクセント情報を返す\n",
    "        norm_text = text_normalize(text)\n",
    "        kata_tone = g2kata_tone(norm_text)\n",
    "        kata_tone_json_str = json.dumps(kata_tone, ensure_ascii=False)\n",
    "    elif tone is None:\n",
    "        kata_tone_json_str = \"\"\n",
    "\n",
    "    if reference_audio_path:\n",
    "        style = \"External Audio\"\n",
    "    logger.info(\n",
    "        f\"Successful inference, took {duration}s | {speaker} | {language}/{sdp_ratio}/{noise_scale}/{noise_scale_w}/{length_scale}/{style}/{style_weight} | {text}\"\n",
    "    )\n",
    "    message = f\"Success, time: {duration} seconds.\"\n",
    "    if wrong_tone_message != \"\":\n",
    "        message = wrong_tone_message + \"\\n\" + message\n",
    "    return message, (sr, audio), kata_tone_json_str\n",
    "\n",
    "\n",
    "def load_voicedata():\n",
    "    print(\"Loading voice data...\")\n",
    "    # voices = []\n",
    "    envoices = []\n",
    "    jpvoices = []\n",
    "    styledict = {}\n",
    "    with open(\"voicelist.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        voc_info = json.load(f)\n",
    "    for name, info in voc_info.items():\n",
    "        if not info[\"enable\"]:\n",
    "            continue\n",
    "        model_path = info[\"model_path\"]\n",
    "        voice_name = info[\"title\"]\n",
    "        speakerid = info[\"speakerid\"]\n",
    "        datasetauthor = info[\"datasetauthor\"]\n",
    "        image = info[\"cover\"]\n",
    "        if not model_path in styledict.keys():\n",
    "            conf = f\"model_assets/{model_path}/config.json\"\n",
    "            hps = utils.get_hparams_from_file(conf)\n",
    "            s2id = hps.data.style2id\n",
    "            styledict[model_path] = s2id.keys()\n",
    "        print(f\"Indexed voice {voice_name}\")\n",
    "        if info[\"primarylang\"] == \"JP\":\n",
    "            jpvoices.append(\n",
    "                (name, model_path, voice_name, speakerid, datasetauthor, image)\n",
    "            )\n",
    "        else:\n",
    "            envoices.append(\n",
    "                (name, model_path, voice_name, speakerid, datasetauthor, image)\n",
    "            )\n",
    "    return [envoices, jpvoices], styledict\n",
    "\n",
    "\n",
    "class Predictor(BasePredictor):\n",
    "    def setup(self) -> None:\n",
    "        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n",
    "        global model_holder  # Declare model_holder as a global variable\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model_holder = ModelHolder(assets_root, self.device)\n",
    "\n",
    "        self.languages = [\"EN\", \"JP\", \"ZH\"]\n",
    "        self.langnames = [\"English\", \"Japanese\"]\n",
    "\n",
    "        model_names = model_holder.model_names\n",
    "        if len(model_names) == 0:\n",
    "            logger.error(f\"No models found. Please place the model in {assets_root}.\")\n",
    "            sys.exit(1)\n",
    "        initial_id = 0\n",
    "        initial_pth_files = model_holder.model_files_dict[model_names[initial_id]]\n",
    "\n",
    "        self.voicedata, self.styledict = load_voicedata()\n",
    "\n",
    "        # Load the initial model\n",
    "        model_holder.load_model_gr(model_names[initial_id], initial_pth_files[0])\n",
    "        print(f\"Loaded initial model: {model_names[initial_id]}\")\n",
    "\n",
    "        # Verify that the model is loaded\n",
    "        if model_holder.current_model is None:\n",
    "            raise RuntimeError(\"Failed to load the initial model.\")\n",
    "        else:\n",
    "            print(f\"Model loaded successfully: {model_holder.current_model.model_path}\")\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        speaker: str = \"MoriCalliope\",  # Default speaker\n",
    "        text_input: str = \"Hello there! This is test audio of a new Hololive text to speech tool.\",\n",
    "        language: str = \"EN\",\n",
    "        reference_audio_path: str = None,\n",
    "        line_split: bool = True,\n",
    "        split_interval: float = 0.5,\n",
    "        style: str = \"Neutral\",\n",
    "        style_weight: int = 5,\n",
    "        use_tone: bool = False,\n",
    "        sdp_ratio: float = 0.2,\n",
    "        noise_scale: float = 0.6,\n",
    "        noise_scale_w: float = 0.8,\n",
    "        length_scale: int = 1,\n",
    "        style_text_weight: float = 0.7,\n",
    "        use_style_text: bool = False,\n",
    "        style_text: str = \"\",\n",
    "    ) -> Path:\n",
    "        # Find the speaker in voicedata\n",
    "        for voice_group in self.voicedata:\n",
    "            for voice in voice_group:\n",
    "                if voice[3] == speaker:\n",
    "                    (\n",
    "                        name,\n",
    "                        model_path,\n",
    "                        voice_name,\n",
    "                        speakerid,\n",
    "                        datasetauthor,\n",
    "                        image,\n",
    "                    ) = voice\n",
    "                    model_name = model_path  # Correctly infer model_name\n",
    "                    model_path = f\"model_assets/{model_path}/{model_path}.safetensors\"\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            raise ValueError(f\"Speaker {speaker} not found in voicedata.\")\n",
    "\n",
    "        # Debugging: Print available speakers and the selected speaker\n",
    "        english_speakers = [\n",
    "            \"MoriCalliope\",\n",
    "            \"TakanashiKiara\",\n",
    "            \"NinomaeInanis\",\n",
    "            \"GawrGura\",\n",
    "            \"AmeliaWatson\",\n",
    "            \"IRyS\",\n",
    "            \"TsukumoSana\",\n",
    "            \"CeresFauna\",\n",
    "            \"OuroKronii\",\n",
    "            \"NanashiMumei\",\n",
    "            \"HakosBaelz\",\n",
    "            \"ShioriNovella\",\n",
    "            \"KosekiBijou\",\n",
    "            \"NerissaRavencroft\",\n",
    "            \"AiraniIofifteen\",\n",
    "            \"KureijiOllie\",\n",
    "            \"AnyaMelfissa\",\n",
    "            \"VestiaZeta\",\n",
    "        ]\n",
    "        japanese_speakers = [\n",
    "            \"TokinoSora\",\n",
    "            \"HoshimachiSuisei\",\n",
    "            \"AZKi\",\n",
    "            \"YozoraMel\",\n",
    "            \"NatsuiroMatsuri\",\n",
    "            \"AkiRosenthal\",\n",
    "            \"AkaiHaato\",\n",
    "            \"MinatoAqua\",\n",
    "            \"NakiriAyame\",\n",
    "            \"NekomataOkayu\",\n",
    "            \"ShiranuiFlare\",\n",
    "            \"ShiroganeNoel\",\n",
    "            \"HoushouMarine\",\n",
    "            \"TokoyamiTowa\",\n",
    "            \"YukihanaLamy\",\n",
    "            \"LaplusDarknesss\",\n",
    "            \"TakaneLui\",\n",
    "            \"HakuiKoyori\",\n",
    "            \"SakamataChloe\",\n",
    "            \"IchijouRirika\",\n",
    "        ]\n",
    "        print(\"Available speakers for English:\")\n",
    "        for speaker in english_speakers:\n",
    "            print(f\" - {speaker}\")\n",
    "\n",
    "        print(\"\\nAvailable speakers for Japanese:\")\n",
    "        for speaker in japanese_speakers:\n",
    "            print(f\" - {speaker}\")\n",
    "\n",
    "        print(f\"\\nSelected speaker ID: {speakerid}\")\n",
    "\n",
    "        # Ensure the types of the parameters match the expected types in tts_fn\n",
    "        text_output, (sr, audio), kata_toneas_json_str = tts_fn(\n",
    "            model_name,\n",
    "            model_path,\n",
    "            text_input,\n",
    "            language,\n",
    "            reference_audio_path if reference_audio_path else None,\n",
    "            float(sdp_ratio),\n",
    "            float(noise_scale),\n",
    "            float(noise_scale_w),\n",
    "            float(length_scale),\n",
    "            bool(line_split),\n",
    "            float(split_interval),\n",
    "            style_text,\n",
    "            float(style_text_weight),\n",
    "            bool(use_style_text),\n",
    "            style,\n",
    "            float(style_weight),\n",
    "            \"\",  # kata_tone_json_str\n",
    "            bool(use_tone),\n",
    "            speakerid,\n",
    "        )\n",
    "\n",
    "        # Check if audio data is valid\n",
    "        if audio is None or len(audio) == 0:\n",
    "            raise ValueError(\"Invalid audio data received from tts_fn\")\n",
    "\n",
    "        # Save the audio output to a file using soundfile\n",
    "        output_path = \"output.wav\"\n",
    "        sf.write(output_path, audio, sr)\n",
    "\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading voice data...\n",
      "Indexed voice Mori Calliope\n",
      "Indexed voice Takanashi Kiara\n",
      "Indexed voice Ninomae Ina'nis\n",
      "Indexed voice Gawr Gura\n",
      "Indexed voice Amelia Watson\n",
      "Indexed voice IRyS\n",
      "Indexed voice Tsukumo Sana\n",
      "Indexed voice Ceres Fauna\n",
      "Indexed voice Ouro Kronii\n",
      "Indexed voice Nanashi Mumei\n",
      "Indexed voice Hakos Baelz\n",
      "Indexed voice Shiori Novella\n",
      "Indexed voice Koseki Bijou\n",
      "Indexed voice Nerissa Ravencroft\n",
      "Indexed voice Airani Iofifteen\n",
      "Indexed voice Kureiji Ollie\n",
      "Indexed voice Anya Melfissa\n",
      "Indexed voice Vestia Zeta\n",
      "Indexed voice Tokino Sora\n",
      "Indexed voice Hoshimachi Suisei\n",
      "Indexed voice AZKi\n",
      "Indexed voice Yozora Mel\n",
      "Indexed voice Natsuiro Matsuri\n",
      "Indexed voice Aki Rosenthal\n",
      "Indexed voice Akai Haato\n",
      "Indexed voice Minato Aqua\n",
      "Indexed voice Nakiri Ayame\n",
      "Indexed voice Nekomata Okayu\n",
      "Indexed voice Shiranui Flare\n",
      "Indexed voice Shirogane Noel\n",
      "Indexed voice Houshou Marine\n",
      "Indexed voice Tokoyami Towa\n",
      "Indexed voice Yukihana Lamy\n",
      "Indexed voice La+ Darknesss\n",
      "Indexed voice Takane Lui\n",
      "Indexed voice Hakui Koyori\n",
      "Indexed voice Sakamata Chloe\n",
      "Indexed voice Ichijou Ririka\n",
      "Loaded initial model: SBV2_HoloJPTest2\n",
      "Model loaded successfully: model_assets/SBV2_HoloJPTest2/SBV2_HoloJPTest2.safetensors\n"
     ]
    }
   ],
   "source": [
    "# Initialize the predictor\n",
    "predictor = Predictor()\n",
    "predictor.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available speakers for English:\n",
      " - MoriCalliope\n",
      " - TakanashiKiara\n",
      " - NinomaeInanis\n",
      " - GawrGura\n",
      " - AmeliaWatson\n",
      " - IRyS\n",
      " - TsukumoSana\n",
      " - CeresFauna\n",
      " - OuroKronii\n",
      " - NanashiMumei\n",
      " - HakosBaelz\n",
      " - ShioriNovella\n",
      " - KosekiBijou\n",
      " - NerissaRavencroft\n",
      " - AiraniIofifteen\n",
      " - KureijiOllie\n",
      " - AnyaMelfissa\n",
      " - VestiaZeta\n",
      "\n",
      "Available speakers for Japanese:\n",
      " - TokinoSora\n",
      " - HoshimachiSuisei\n",
      " - AZKi\n",
      " - YozoraMel\n",
      " - NatsuiroMatsuri\n",
      " - AkiRosenthal\n",
      " - AkaiHaato\n",
      " - MinatoAqua\n",
      " - NakiriAyame\n",
      " - NekomataOkayu\n",
      " - ShiranuiFlare\n",
      " - ShiroganeNoel\n",
      " - HoushouMarine\n",
      " - TokoyamiTowa\n",
      " - YukihanaLamy\n",
      " - LaplusDarknesss\n",
      " - TakaneLui\n",
      " - HakuiKoyori\n",
      " - SakamataChloe\n",
      " - IchijouRirika\n",
      "\n",
      "Selected speaker ID: MoriCalliope\n",
      "[!] model_name: SBV2_HoloLow\n",
      "[!] model_path: model_assets/SBV2_HoloLow/SBV2_HoloLow.safetensors\n",
      "[!] text: Hello there! This is test audio of a new Hololive text to speech tool.\n",
      "[!] language: EN\n",
      "[!] reference_audio_path: None\n",
      "[!] sdp_ratio: 0.2\n",
      "[!] noise_scale: 0.6\n",
      "[!] noise_scale_w: 0.8\n",
      "[!] length_scale: 1.0\n",
      "[!] line_split: True\n",
      "[!] split_interval: 0.5\n",
      "[!] assist_text: \n",
      "[!] assist_text_weight: 0.7\n",
      "[!] use_assist_text: False\n",
      "[!] style: Neutral\n",
      "[!] style_weight: 5.0\n",
      "[!] kata_tone_json_str: \n",
      "[!] use_tone: False\n",
      "[!] speaker: MoriCalliope\n",
      "\u001b[32m06-03 15:12:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| 2094862345.py:89 | Swapped to model 'SBV2_HoloLow'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsakib/miniconda3/envs/hsbv2/lib/python3.11/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m06-03 15:12:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| 2094862345.py:161 | Successful inference, took 6.403847s | MoriCalliope | EN/0.2/0.6/0.8/1.0/Neutral/5.0 | Hello there! This is test audio of a new Hololive text to speech tool.\n",
      "Output audio saved to: output.wav\n"
     ]
    }
   ],
   "source": [
    "# Call the predict method with the speaker \"MoriCalliope\"\n",
    "\n",
    "output = predictor.predict(\n",
    "    text_input=\"Hello there! This is test audio of a new Hololive text to speech tool.\",\n",
    "    line_split=True,\n",
    "    split_interval=0.5,\n",
    "    language=\"EN\",\n",
    "    sdp_ratio=0.2,\n",
    "    noise_scale=0.6,\n",
    "    noise_scale_w=0.8,\n",
    "    length_scale=1.0,\n",
    "    use_style_text=False,\n",
    "    style_text=\"\",\n",
    "    style_text_weight=0.7,\n",
    "    style=\"Neutral\",\n",
    "    style_weight=5.0,\n",
    "    reference_audio_path=\"\",\n",
    "    use_tone=False,\n",
    "    speaker=\"MoriCalliope\",  # Use the speakerid for Calli\n",
    ")\n",
    "\n",
    "print(f\"Output audio saved to: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SBV2_HoloLow': dict_keys(['Neutral', 'Kronii', 'NerissaLaugh', 'Calli', 'Nerissa']),\n",
       " 'SBV2_TakanashiKiara': dict_keys(['Neutral', 'Normal', 'Japanese', 'Happy', 'Reading', 'Excited']),\n",
       " 'SBV2_HoloHi': dict_keys(['Neutral', 'Fauna', 'Amelia', 'MumeiLaugh', 'Shiori', 'IRyS', 'Ina', 'Gura', 'Mumei', 'ShioriLaugh']),\n",
       " 'SBV2_HoloAus': dict_keys(['Neutral', 'Sana', 'Baelz1', 'Baelz2', 'BaelzShouting']),\n",
       " 'SBV2_KosekiBijou': dict_keys(['Neutral', 'Normal', 'Scared', 'Angry', 'Excited']),\n",
       " 'SBV2_HoloESL': dict_keys(['Neutral', 'Anya', 'IofiLoud', 'Iofi']),\n",
       " 'SBV2_HoloIDFlu': dict_keys(['Neutral', 'ZetaSoft', 'Zeta', 'ZetaLoud', 'Ollie']),\n",
       " 'SBV2_HoloJPTest2': dict_keys(['Neutral', 'Koyori', 'Chloe', 'Lamy', 'Aqua', 'Sora', 'Towa', 'Suisei', 'Ayame']),\n",
       " 'SBV2_HoloJPTest2.5': dict_keys(['Neutral', 'Haato', 'Matsuri', 'Mel', 'Aki', 'Lui', 'AZKi']),\n",
       " 'SBV2_HoloJPTest': dict_keys(['Neutral', 'Flare', 'Ririka', 'Laplus', 'Noel', 'Okayu', 'Marine'])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [elem for elem in predictor.voicedata]\n",
    "\n",
    "predictor.styledict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsbv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
